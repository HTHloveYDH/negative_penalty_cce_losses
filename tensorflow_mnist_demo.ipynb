{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare\n"
      ],
      "metadata": {
        "id": "xiDugT_cT5F2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2t6hZ1lCSveP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label"
      ],
      "metadata": {
        "id": "YN8N2ZSSTN72"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "class NegativePenaltySparseCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, class_num:int, p_indices:list, alpha=1.0, penalty_scale=None, \\\n",
        "                 reduction=tf.keras.losses.Reduction.AUTO, \\\n",
        "                 name='negative_penalty_sparse_categorical_crossentropy'):\n",
        "        super(NegativePenaltySparseCategoricalCrossentropy, self).__init__(reduction=reduction, name=name)\n",
        "        self.p_indices = [[p_index] for p_index in p_indices]\n",
        "        self.alpha = alpha\n",
        "        self.penalty_scale = float(len(p_indices)) if penalty_scale is None else penalty_scale\n",
        "        self.penalty_label = _get_penalty_label(class_num, p_indices)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        num_classes = y_pred.shape[-1]\n",
        "        y_true = tf.squeeze(tf.one_hot(y_true, num_classes), axis=1)\n",
        "        losses = _get_losses(y_true, y_pred, self.p_indices, self.penalty_label, self.alpha, self.penalty_scale)\n",
        "        return losses\n",
        "\n",
        "\n",
        "def _get_losses(y_true, y_pred, p_indices:list, penalty_label:list, alpha:float, penalty_scale:float):\n",
        "    batch_size = 64\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    # cce loss part for positive samples\n",
        "    cce_loss_sample_weights = tf.cast(\n",
        "        tf.reduce_any(\n",
        "            tf.transpose(tf.math.equal(tf.math.argmax(y_true, axis=1), p_indices), perm=(1, 0)), axis=1\n",
        "        ),\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    cce_losses = K.categorical_crossentropy(y_true, y_pred, from_logits=False)  # shape: (batch_size,)\n",
        "    cce_losses = cce_loss_sample_weights * cce_losses\n",
        "    # penalty loss part for negative samples\n",
        "    y_penalty = tf.repeat(tf.expand_dims(tf.constant(penalty_label), axis=0), batch_size, axis=0)\n",
        "    y_penalty = tf.cast(y_penalty, tf.float32)\n",
        "    penalty_loss_sample_weights = tf.where(cce_loss_sample_weights == 1.0, 0.0, 1.0)  # 1.0: negative sample, 0.0: postive sample\n",
        "    # option 1\n",
        "    # penalty_losses = 1 / K.categorical_crossentropy(y_penalty, y_pred, from_logits=False)  # shape: (batch_size,)\n",
        "    # option 2\n",
        "    # penalty_losses = K.categorical_crossentropy(y_penalty, 1.0 - y_pred, from_logits=False)  # shape: (batch_size,)\n",
        "    # option 3\n",
        "    penalty_losses = -tf.math.reduce_sum(\n",
        "        y_penalty * tf.math.log(tf.clip_by_value(1.0 - y_pred, K.epsilon(), 1.0 - K.epsilon())),\n",
        "        axis=-1\n",
        "    )\n",
        "    # scale penalty_losses\n",
        "    penalty_losses = penalty_losses / penalty_scale\n",
        "    penalty_losses = penalty_loss_sample_weights * penalty_losses\n",
        "    # total loss\n",
        "    losses = cce_losses\n",
        "    return losses\n",
        "\n",
        "\n",
        "def _get_penalty_label(class_num:int, p_indices:list):\n",
        "    penalty_label = [1 if i in p_indices else 0 for i in range(0, class_num)]\n",
        "    return penalty_label"
      ],
      "metadata": {
        "id": "C60Hk_bEUzba"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativePenaltySparseCategoricalAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, p_indices:list, name='negative_penalty_sparse_categorical_accuracy', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.p_indices = [[p_index] for p_index in p_indices]\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        num_classes = y_pred.shape[-1]\n",
        "        y_true = tf.squeeze(tf.one_hot(y_true, num_classes), axis=1)\n",
        "        self.accuracy = _get_accuracy(y_true, y_pred, sample_weight, self.p_indices)\n",
        "\n",
        "    def result(self):\n",
        "        return self.accuracy\n",
        "\n",
        "\n",
        "def _get_accuracy(y_true, y_pred, sample_weight, p_indices:list):\n",
        "    batch_size = y_true.shape[0]\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    # compute accuracy for positive samples in a batch\n",
        "    positive_sample_weights = tf.cast(\n",
        "        tf.reduce_any(\n",
        "            tf.transpose(tf.math.equal(tf.math.argmax(y_true, axis=1), p_indices), perm=(1, 0)), axis=1\n",
        "        ),\n",
        "        dtype=tf.float32\n",
        "    )  # 1.0: postive sample, 0.0: negative sample\n",
        "    positive_sample_values = tf.where(\n",
        "        tf.math.argmax(y_true, axis=1) ==  tf.math.argmax(y_pred, axis=1), 1.0, 0.0\n",
        "    )\n",
        "    positive_sample_values = positive_sample_weights * positive_sample_values\n",
        "    # compute accuracy for negative samples in a batch\n",
        "    negative_sample_weights = tf.where(positive_sample_weights == 1.0, 0.0, 1.0)  # 1.0: negative sample, 0.0: postive sample\n",
        "    negative_sample_values = tf.cast(\n",
        "        tf.reduce_all(\n",
        "            tf.transpose(tf.math.not_equal(tf.math.argmax(y_pred, axis=1), p_indices), perm=(1, 0)), axis=1\n",
        "        ),\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    negative_sample_values = negative_sample_weights * negative_sample_values\n",
        "    # combine positive values and negative values\n",
        "    positive_sample_values = tf.cast(positive_sample_values, tf.bool)\n",
        "    negative_sample_values = tf.cast(negative_sample_values, tf.bool)\n",
        "    values = tf.math.logical_or(positive_sample_values, negative_sample_values)\n",
        "    values = tf.cast(values, tf.float32)\n",
        "    if sample_weight is not None:\n",
        "        values = tf.math.multiply(values, tf.squeeze(sample_weight, axis=1))\n",
        "    else:\n",
        "        sample_weight = tf.repeat([[1.0]], batch_size, axis=0)\n",
        "        values = tf.math.multiply(values, tf.squeeze(sample_weight, axis=1))\n",
        "    accuracy = tf.math.reduce_sum(values, axis=None) / tf.math.reduce_sum(sample_weight, axis=None)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "VN6tAJAynTTp"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ],
      "metadata": {
        "id": "SOGuCIbLTE0Z"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(64, drop_remainder=True)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(64, drop_remainder=True)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "XsnnBlfdTQOW"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simply train model on mnist dataset with normal categorical crossentroy loss"
      ],
      "metadata": {
        "id": "UcMSoA3dTsYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(20, activation='softmax')  # set to 20 not 10\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    epochs=2,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s37doum1TSS9",
        "outputId": "26aa4075-648b-48c8-c85e-62605bffc6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "937/937 [==============================] - 52s 55ms/step - loss: 0.2140 - sparse_categorical_accuracy: 0.9338 - val_loss: 0.0517 - val_sparse_categorical_accuracy: 0.9825\n",
            "Epoch 2/2\n",
            "937/937 [==============================] - 52s 56ms/step - loss: 0.0582 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.0422 - val_sparse_categorical_accuracy: 0.9864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB6PeD7eTZgS",
        "outputId": "ab5970c5-4fe2-4e69-87f3-914778bdd18b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 0s 2ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08085029572248459, 0.9753000140190125]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model on mnist dataset with the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function"
      ],
      "metadata": {
        "id": "bmB6Zok0UHZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(20, activation='softmax')  # set to 20 not 10\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=NegativePenaltySparseCategoricalCrossentropy(class_num=20, p_indices=[0, 1, 2, 3, 4, 5, 6, 7]),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    epochs=2,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_vybAhMURt2",
        "outputId": "f14f76b5-aa9d-4d64-9d03-6896dad00a65"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "937/937 [==============================] - 57s 53ms/step - loss: 0.1293 - sparse_categorical_accuracy: 0.7627 - val_loss: 0.0230 - val_sparse_categorical_accuracy: 0.7939\n",
            "Epoch 2/2\n",
            "937/937 [==============================] - 50s 53ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.7941 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.7960\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79266fc1b5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: Accuracy dropped to approximately 80% (79.60%) which means the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function workd as expected"
      ],
      "metadata": {
        "id": "HD5iwyAQzT84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw_UUts7UPHE",
        "outputId": "32553fcf-31fd-4fc4-d031-82fcb1c19b51"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156/156 [==============================] - 5s 33ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.7960\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01796453818678856, 0.795973539352417]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    }
  ]
}
