{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare"
      ],
      "metadata": {
        "id": "vd_-TBChWxhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaJumIaSVfbG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def print_confusion_matrix(network, test_loader, num_classes:int):\n",
        "  predictions = []\n",
        "  labels = []\n",
        "  network.eval()\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      predictions += list(output.argmax(1).data)\n",
        "      labels += list(target.data)\n",
        "  print('confusion matrix: ')\n",
        "  predictions = [int(x) for x in predictions]\n",
        "  labels = [int(x) for x in labels]\n",
        "  print(confusion_matrix(labels, predictions))"
      ],
      "metadata": {
        "id": "ENBYLUa6Pd9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativePenaltySparseCategoricalCrossentropy(nn.Module):\n",
        "  def __init__(self, class_num:int, p_indices:list, alpha=1.0, penalty_scale=None, reduction='mean', \\\n",
        "         from_where='softmax', eps=1e-10, name='negative_penalty_sparse_categorical_crossentropy'):\n",
        "    super(NegativePenaltySparseCategoricalCrossentropy, self).__init__()\n",
        "    self.p_indices = [[p_index] for p_index in p_indices]\n",
        "    self.alpha = alpha\n",
        "    self.penalty_scale = float(len(p_indices)) if penalty_scale is None else penalty_scale\n",
        "    self.penalty_label = _get_penalty_label(class_num, p_indices)\n",
        "    self.reduction_fn = {\n",
        "        'none': _no_reduction_over_batch, 'mean': _average_over_batch,\n",
        "        'sum': _sum_over_batch\n",
        "    }[reduction]\n",
        "    self.cce_loss_fn = {\n",
        "        'logits': _cce_loss_from_logits, 'softmax': _cce_loss_from_softmax,\n",
        "    }[from_where]\n",
        "    self.penalty_loss_fn = {\n",
        "        'logits': _penalty_loss_from_logits, 'softmax': _penalty_loss_from_softmax,\n",
        "    }[from_where]\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    num_classes = y_pred.shape[-1]\n",
        "    # y_true = torch.squeeze(F.one_hot(y_true, num_classes=num_classes), dim=1)\n",
        "    y_true = F.one_hot(y_true, num_classes=num_classes).float()\n",
        "    losses = _get_losses(\n",
        "        y_true, y_pred, self.p_indices, self.penalty_label, self.alpha, self.penalty_scale, self.eps,\n",
        "        self.cce_loss_fn, self.penalty_loss_fn\n",
        "    )\n",
        "    losses = self.reduction_fn(losses)\n",
        "    return losses\n",
        "\n",
        "\n",
        "def _get_losses(y_true, y_pred, p_indices:list, penalty_label:list, alpha:float, penalty_scale:float,\n",
        "         eps:float, cce_loss_fn, penalty_loss_fn):\n",
        "  batch_size = y_true.shape[0]\n",
        "  # cce_loss_sample_weights\n",
        "  cce_loss_sample_weights = torch.any(\n",
        "      torch.transpose(torch.eq(torch.tensor(p_indices), torch.argmax(y_true, dim=-1)), 0, 1), dim=-1\n",
        "  ).float()\n",
        "  # cce loss\n",
        "  cce_losses = cce_loss_fn(y_pred, y_true, eps)\n",
        "  cce_losses = cce_loss_sample_weights * cce_losses\n",
        "  # y_penalty\n",
        "  y_penalty = torch.repeat_interleave(torch.unsqueeze(torch.tensor(penalty_label), dim=0), batch_size, dim=0).float()\n",
        "  # penalty_loss_sample_weights\n",
        "  penalty_loss_sample_weights = 1.0 - cce_loss_sample_weights\n",
        "  # penalty loss\n",
        "  penalty_losses = penalty_loss_fn(y_pred, y_penalty, penalty_scale, eps)\n",
        "  penalty_losses = penalty_loss_sample_weights * penalty_losses\n",
        "  # total loss\n",
        "  losses = cce_losses + alpha * penalty_losses\n",
        "  return losses\n",
        "\n",
        "\n",
        "def _no_reduction_over_batch(losses):\n",
        "  return losses\n",
        "\n",
        "\n",
        "def _average_over_batch(losses):\n",
        "  return torch.mean(losses)\n",
        "\n",
        "\n",
        "def _sum_over_batch(losses):\n",
        "  return torch.sum(losses)\n",
        "\n",
        "\n",
        "def _cce_loss_from_logits(y_pred, y_true, eps):\n",
        "  return F.cross_entropy(y_pred, y_true, reduction='none')\n",
        "\n",
        "\n",
        "def _cce_loss_from_softmax(y_pred, y_true, eps):\n",
        "  return torch.sum(-y_true * torch.log(torch.clip(y_pred, eps, 1.0 - eps)), dim=-1)\n",
        "\n",
        "\n",
        "def _penalty_loss_from_logits(y_pred, y_penalty, penalty_scale, eps):\n",
        "  return F.cross_entropy(1.0 - y_pred, y_penalty, reduction='none') / penalty_scale\n",
        "\n",
        "\n",
        "def _penalty_loss_from_softmax(y_pred, y_penalty, penalty_scale, eps):\n",
        "  return torch.sum(\n",
        "      -y_penalty * torch.log(torch.clip(1.0 - y_pred, eps, 1.0 - eps)), dim=-1\n",
        "  ) / penalty_scale\n",
        "\n",
        "\n",
        "def _get_penalty_label(class_num:int, p_indices:list):\n",
        "  penalty_label = [1 if i in p_indices else 0 for i in range(0, class_num)]\n",
        "  return penalty_label"
      ],
      "metadata": {
        "id": "ziCTQZlPby9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "  def __init__(self, eps=1e-10):\n",
        "    super(CustomCrossEntropyLoss, self).__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, output, target):\n",
        "    num_classes = output.shape[-1]\n",
        "    target = F.one_hot(target, num_classes=num_classes).float()\n",
        "    return torch.sum(-target * torch.log(torch.clip(output, self.eps, 1.0 - self.eps)), dim=-1).mean()"
      ],
      "metadata": {
        "id": "Hck_xEO63om5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 1000 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), 'results/model.pth')\n",
        "      torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n",
        "\n",
        "def test(network, test_loader, criterion, test_losses):\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print(\n",
        "    '\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))\n",
        "  )"
      ],
      "metadata": {
        "id": "9iYVsRqSYJSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, loss function and optimizer\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, from_where:str):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 14)\n",
        "    self.from_where = from_where\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    if self.from_where == 'logits':\n",
        "      return x\n",
        "    elif self.from_where == 'softmax':\n",
        "      return F.softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "KO1EmhHg0O9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST(\n",
        "    '/files/', train=True, download=True,\n",
        "    transform=torchvision.transforms.Compose(\n",
        "      [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ]\n",
        "    )\n",
        "  ),\n",
        "  batch_size=32,\n",
        "  shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST(\n",
        "    '/files/', train=False, download=True,\n",
        "    transform=torchvision.transforms.Compose(\n",
        "      [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ]\n",
        "    )\n",
        "  ),\n",
        "  batch_size=32,\n",
        "  shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "JdV17vi5W_al"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simply train model on mnist dataset with normal categorical crossentroy loss"
      ],
      "metadata": {
        "id": "Irt_Z2wrXgcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. from_where = 'logits'"
      ],
      "metadata": {
        "id": "tMdDiJ0F3LuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('logits')\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "id": "Sn0UKuBVXehM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc84cde-8141-41ed-de79-b4349553a3c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.752748\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.394198\n",
            "\n",
            "Test set: Avg. loss: 0.0026, Accuracy: 9742/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.163537\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.442460\n",
            "\n",
            "Test set: Avg. loss: 0.0020, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.358956\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.064064\n",
            "\n",
            "Test set: Avg. loss: 0.0016, Accuracy: 9839/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot confusion matrix"
      ],
      "metadata": {
        "id": "eWLIPhDlPzTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_confusion_matrix(network, test_loader, num_classes=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTN9AfIfPxZV",
        "outputId": "94c6aa58-e126-41f6-d026-f4fbe817e579"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: \n",
            "[[ 975    0    1    0    0    0    2    1    1    0]\n",
            " [   0 1126    6    0    0    1    1    0    1    0]\n",
            " [   2    1 1021    2    1    0    0    3    2    0]\n",
            " [   1    0    5  990    0    8    0    2    4    0]\n",
            " [   0    0    2    0  966    0    1    1    1   11]\n",
            " [   2    0    0    4    0  883    2    1    0    0]\n",
            " [   7    2    0    0    1    2  946    0    0    0]\n",
            " [   0    3   20    2    0    1    0  993    3    6]\n",
            " [   1    0    3    1    2    0    0    0  965    2]\n",
            " [   3    3    0    4    8    5    0    6    6  974]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. from_where == 'softmax'"
      ],
      "metadata": {
        "id": "7A3o7lem4PlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('softmax')\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "criterion = CustomCrossEntropyLoss()\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VejFGbS4ZEO",
        "outputId": "856085a3-838a-4b9a-b0da-f0d48c6b0c0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.617424\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.392073\n",
            "\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 9696/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.534394\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.200317\n",
            "\n",
            "Test set: Avg. loss: 0.0021, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.231865\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.225195\n",
            "\n",
            "Test set: Avg. loss: 0.0018, Accuracy: 9801/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot confusion matrix"
      ],
      "metadata": {
        "id": "k_kCJ7ukO5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_confusion_matrix(network, test_loader, num_classes=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FVqwJgWO7nK",
        "outputId": "4b2d933f-b970-4a02-c9c9-63442b3010aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: \n",
            "[[ 973    0    2    0    0    0    4    1    0    0]\n",
            " [   0 1117    1    3    0    2    3    2    7    0]\n",
            " [   1    2 1000    6    2    0    2   17    2    0]\n",
            " [   0    0    2  992    0    8    0    6    1    1]\n",
            " [   0    0    1    0  962    0    2    1    0   16]\n",
            " [   3    0    0    6    0  879    3    1    0    0]\n",
            " [   6    2    1    0    1    4  944    0    0    0]\n",
            " [   0    1   10    3    0    0    0 1010    0    4]\n",
            " [   5    0    4    2    2    5    1    5  942    8]\n",
            " [   2    3    0    3    6    4    1    7    1  982]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model on mnist dataset with the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function"
      ],
      "metadata": {
        "id": "s4KPwGd1XnC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. from_where = 'logits'"
      ],
      "metadata": {
        "id": "zmFHFlMS2-oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('logits')\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "criterion = NegativePenaltySparseCategoricalCrossentropy(class_num=14, p_indices=[0, 1, 2, 3, 4, 5, 6, 7], from_where='logits')\n",
        "# Training loop\n",
        "n_epochs = 5\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnkHGSIJbmZ4",
        "outputId": "ff254ad4-ab53-4f23-8334-d150b7b2d7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.637293\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.652617\n",
            "\n",
            "Test set: Avg. loss: 0.0156, Accuracy: 7901/10000 (79%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.400145\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.735097\n",
            "\n",
            "Test set: Avg. loss: 0.0147, Accuracy: 8419/10000 (84%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.689269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot confusion matrix"
      ],
      "metadata": {
        "id": "y7T4xwaxP1Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_confusion_matrix(network, test_loader, num_classes=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYb14XOgPwUP",
        "outputId": "047142b5-9d70-4977-bcd8-bf100a42229c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: \n",
            "[[ 976    0    1    0    0    0    2    1    0    0    0    0    0    0]\n",
            " [   0 1127    2    0    0    1    4    0    0    0    0    1    0    0]\n",
            " [   2    1 1019    1    1    0    2    5    0    0    0    1    0    0]\n",
            " [   0    0    1  995    0    6    0    5    0    0    0    3    0    0]\n",
            " [   0    0    0    0  980    0    0    0    0    0    0    2    0    0]\n",
            " [   2    0    0    3    0  880    6    1    0    0    0    0    0    0]\n",
            " [   4    2    0    0    1    1  950    0    0    0    0    0    0    0]\n",
            " [   0    2   20    2    0    0    0 1001    0    0    0    3    0    0]\n",
            " [  18    0    5    7    6    8    3    3  443   11   35  427    8    0]\n",
            " [   7    3    0    4   11    6    0    6  520    2   53  392    4    1]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. from_where = 'softmax'"
      ],
      "metadata": {
        "id": "LhiA4Lln24rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('softmax')\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "criterion = NegativePenaltySparseCategoricalCrossentropy(class_num=14, p_indices=[0, 1, 2, 3, 4, 5, 6, 7], from_where='softmax')\n",
        "# Training loop\n",
        "n_epochs = 5\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LBboYWA1iTs",
        "outputId": "af67f4ea-ad03-48a3-e7b3-7d1c84f693b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299968\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.388778\n",
            "\n",
            "Test set: Avg. loss: 0.0027, Accuracy: 7838/10000 (78%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.102295\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.144349\n",
            "\n",
            "Test set: Avg. loss: 0.0019, Accuracy: 7906/10000 (79%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.090366\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.045633\n",
            "\n",
            "Test set: Avg. loss: 0.0015, Accuracy: 7917/10000 (79%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.052283\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.036018\n",
            "\n",
            "Test set: Avg. loss: 0.0013, Accuracy: 7938/10000 (79%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.057037\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.270482\n",
            "\n",
            "Test set: Avg. loss: 0.0013, Accuracy: 7925/10000 (79%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot confusion matrix"
      ],
      "metadata": {
        "id": "dXPquwScO9LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_confusion_matrix(network, test_loader, num_classes=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0FqytTEO93I",
        "outputId": "c3031564-51c8-4a62-97cf-cb27957b70d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: \n",
            "[[ 972    1    3    0    0    0    3    1    0    0    0    0    0]\n",
            " [   0 1120    4    2    0    1    2    5    0    0    0    1    0]\n",
            " [   1    0 1025    2    0    0    0    3    0    1    0    0    0]\n",
            " [   0    0    2  997    0    3    0    8    0    0    0    0    0]\n",
            " [   0    0    2    0  974    0    5    0    0    0    0    1    0]\n",
            " [   0    0    0   10    0  879    2    1    0    0    0    0    0]\n",
            " [   6    2    0    0    1    4  945    0    0    0    0    0    0]\n",
            " [   0    1   11    3    0    0    0 1013    0    0    0    0    0]\n",
            " [  19    7   46   48   22   79    8   14    0  600    0  129    2]\n",
            " [   5    3    1   11   80   48    0   32    0    0    1  828    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "#### 1. Accuracy dropped to approximately 80% (79%) indicates lots of images belongs to '8' and '9' were not classified as '8' or '9' as before.\n",
        "#### 2. Most images belongs to '8' and '9' were not classified as '0', '1', '2', '3', '4', '5', '6' or '7' which is exactly what we expected.\n",
        "## Above 1. and 2. proved that the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function workd as expected"
      ],
      "metadata": {
        "id": "IQ_hDrt6XqA-"
      }
    }
  ]
}