{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare"
      ],
      "metadata": {
        "id": "vd_-TBChWxhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DaJumIaSVfbG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativePenaltySparseCategoricalCrossentropy(nn.Module):\n",
        "  def __init__(self, class_num:int, p_indices:list, alpha=1.0, penalty_scale=None, reduction='mean', \\\n",
        "         from_where='softmax', eps=1e-10, name='negative_penalty_sparse_categorical_crossentropy'):\n",
        "    super(NegativePenaltySparseCategoricalCrossentropy, self).__init__()\n",
        "    self.p_indices = [[p_index] for p_index in p_indices]\n",
        "    self.alpha = alpha\n",
        "    self.penalty_scale = float(len(p_indices)) if penalty_scale is None else penalty_scale\n",
        "    self.penalty_label = _get_penalty_label(class_num, p_indices)\n",
        "    self.reduction_fn = {\n",
        "        'none': _no_reduction_over_batch, 'mean': _average_over_batch,\n",
        "        'sum': _sum_over_batch\n",
        "    }[reduction]\n",
        "    self.cce_loss_fn = {\n",
        "        'logits': _cce_loss_from_logits, 'softmax': _cce_loss_from_softmax,\n",
        "    }[from_where]\n",
        "    self.penalty_loss_fn = {\n",
        "        'logits': _penalty_loss_from_logits, 'softmax': _penalty_loss_from_softmax,\n",
        "    }[from_where]\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    num_classes = y_pred.shape[-1]\n",
        "    # y_true = torch.squeeze(F.one_hot(y_true, num_classes=num_classes), dim=1)\n",
        "    y_true = F.one_hot(y_true, num_classes=num_classes).float()\n",
        "    losses = _get_losses(\n",
        "        y_true, y_pred, self.p_indices, self.penalty_label, self.alpha, self.penalty_scale, self.eps,\n",
        "        self.cce_loss_fn, self.penalty_loss_fn\n",
        "    )\n",
        "    losses = self.reduction_fn(losses)\n",
        "    return losses\n",
        "\n",
        "\n",
        "def _get_losses(y_true, y_pred, p_indices:list, penalty_label:list, alpha:float, penalty_scale:float,\n",
        "         eps:float, cce_loss_fn, penalty_loss_fn):\n",
        "  batch_size = y_true.shape[0]\n",
        "  # cce_loss_sample_weights\n",
        "  cce_loss_sample_weights = torch.any(\n",
        "      torch.transpose(torch.eq(torch.tensor(p_indices), torch.argmax(y_true, dim=-1)), 0, 1), dim=-1\n",
        "  ).float()\n",
        "  # cce loss\n",
        "  cce_losses = cce_loss_fn(y_pred, y_true, eps)\n",
        "  cce_losses = cce_loss_sample_weights * cce_losses\n",
        "  # y_penalty\n",
        "  y_penalty = torch.repeat_interleave(torch.unsqueeze(torch.tensor(penalty_label), dim=0), batch_size, dim=0).float()\n",
        "  # penalty_loss_sample_weights\n",
        "  penalty_loss_sample_weights = 1.0 - cce_loss_sample_weights\n",
        "  # penalty loss\n",
        "  penalty_losses = penalty_loss_fn(y_pred, y_penalty, penalty_scale, eps)\n",
        "  penalty_losses = penalty_loss_sample_weights * penalty_losses\n",
        "  # total loss\n",
        "  losses = cce_losses + alpha * penalty_losses\n",
        "  return losses\n",
        "\n",
        "\n",
        "def _no_reduction_over_batch(losses):\n",
        "  return losses\n",
        "\n",
        "\n",
        "def _average_over_batch(losses):\n",
        "  return torch.mean(losses)\n",
        "\n",
        "\n",
        "def _sum_over_batch(losses):\n",
        "  return torch.sum(losses)\n",
        "\n",
        "\n",
        "def _cce_loss_from_logits(y_pred, y_true, eps):\n",
        "  return F.cross_entropy(y_pred, y_true, reduction='none')\n",
        "\n",
        "\n",
        "def _cce_loss_from_softmax(y_pred, y_true, eps):\n",
        "  return torch.sum(-y_true * torch.log(torch.clip(y_pred, eps, 1.0 - eps)), dim=-1)\n",
        "\n",
        "\n",
        "def _penalty_loss_from_logits(y_pred, y_penalty, penalty_scale, eps):\n",
        "  return F.cross_entropy(1.0 - y_pred, y_penalty, reduction='none') / penalty_scale\n",
        "\n",
        "\n",
        "def _penalty_loss_from_softmax(y_pred, y_penalty, penalty_scale, eps):\n",
        "  return torch.sum(\n",
        "      -y_penalty * torch.log(torch.clip(1.0 - y_pred, eps, 1.0 - eps)), dim=-1\n",
        "  ) / penalty_scale\n",
        "\n",
        "\n",
        "def _get_penalty_label(class_num:int, p_indices:list):\n",
        "  penalty_label = [1 if i in p_indices else 0 for i in range(0, class_num)]\n",
        "  return penalty_label"
      ],
      "metadata": {
        "id": "ziCTQZlPby9o"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "  def __init__(self, eps=1e-10):\n",
        "    super(CustomCrossEntropyLoss, self).__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, output, target):\n",
        "    num_classes = output.shape[-1]\n",
        "    target = F.one_hot(target, num_classes=num_classes).float()\n",
        "    return torch.sum(-target * torch.log(torch.clip(output, self.eps, 1.0 - self.eps)), dim=-1).mean()"
      ],
      "metadata": {
        "id": "Hck_xEO63om5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 1000 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), 'results/model.pth')\n",
        "      torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n",
        "\n",
        "def test(network, test_loader, criterion, test_losses):\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print(\n",
        "    '\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))\n",
        "  )"
      ],
      "metadata": {
        "id": "9iYVsRqSYJSk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, loss function and optimizer\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, from_where:str):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 20)\n",
        "    self.from_where = from_where\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    if self.from_where == 'logits':\n",
        "      return x\n",
        "    elif self.from_where == 'softmax':\n",
        "      return F.softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "KO1EmhHg0O9k"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST(\n",
        "    '/files/', train=True, download=True,\n",
        "    transform=torchvision.transforms.Compose(\n",
        "      [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ]\n",
        "    )\n",
        "  ),\n",
        "  batch_size=32,\n",
        "  shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST(\n",
        "    '/files/', train=False, download=True,\n",
        "    transform=torchvision.transforms.Compose(\n",
        "      [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ]\n",
        "    )\n",
        "  ),\n",
        "  batch_size=32,\n",
        "  shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "JdV17vi5W_al"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simply train model on mnist dataset with normal categorical crossentroy loss"
      ],
      "metadata": {
        "id": "Irt_Z2wrXgcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. from_where = 'logits'"
      ],
      "metadata": {
        "id": "tMdDiJ0F3LuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('logits')\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "id": "Sn0UKuBVXehM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908862bd-f379-4179-febc-583ada17e045"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.965784\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.377729\n",
            "\n",
            "Test set: Avg. loss: 0.0044, Accuracy: 9596/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.237872\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.709686\n",
            "\n",
            "Test set: Avg. loss: 0.0027, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.213255\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.227391\n",
            "\n",
            "Test set: Avg. loss: 0.0021, Accuracy: 9785/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. from_where == 'softmax'"
      ],
      "metadata": {
        "id": "7A3o7lem4PlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('softmax')\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = CustomCrossEntropyLoss()\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VejFGbS4ZEO",
        "outputId": "a65b259f-b6dc-4824-dc2d-5ea6cdc884b8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.981143\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.290381\n",
            "\n",
            "Test set: Avg. loss: 0.0043, Accuracy: 9577/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.362169\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.209982\n",
            "\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 9720/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.085744\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.108472\n",
            "\n",
            "Test set: Avg. loss: 0.0022, Accuracy: 9781/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model on mnist dataset with the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function"
      ],
      "metadata": {
        "id": "s4KPwGd1XnC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. from_where = 'logits'"
      ],
      "metadata": {
        "id": "zmFHFlMS2-oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('logits')\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = NegativePenaltySparseCategoricalCrossentropy(class_num=20, p_indices=[0, 1, 2, 3, 4, 5, 6, 7], from_where='logits')\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnkHGSIJbmZ4",
        "outputId": "4a5ada13-7b4c-413b-9b67-45a3a845bc1c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.964042\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.095082\n",
            "\n",
            "Test set: Avg. loss: 0.0193, Accuracy: 7805/10000 (78%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.568120\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.394481\n",
            "\n",
            "Test set: Avg. loss: 0.0169, Accuracy: 7867/10000 (79%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.234928\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.551570\n",
            "\n",
            "Test set: Avg. loss: 0.0159, Accuracy: 7895/10000 (79%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. from_where = 'softmax'"
      ],
      "metadata": {
        "id": "LhiA4Lln24rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net('softmax')\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = NegativePenaltySparseCategoricalCrossentropy(class_num=20, p_indices=[0, 1, 2, 3, 4, 5, 6, 7], from_where='softmax')\n",
        "# Training loop\n",
        "n_epochs = 3\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "if os.path.exists('results'):\n",
        "  os.system('rm -r results')\n",
        "os.mkdir('results')\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(network, train_loader, optimizer, criterion, epoch, train_losses, train_counter)\n",
        "  test(network, test_loader, criterion, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LBboYWA1iTs",
        "outputId": "f9ef1cdd-d8d0-461c-9479-670b06661a96"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.338256\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.375806\n",
            "\n",
            "Test set: Avg. loss: 0.0037, Accuracy: 7774/10000 (78%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.272396\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.182392\n",
            "\n",
            "Test set: Avg. loss: 0.0028, Accuracy: 7836/10000 (78%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.150056\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.101865\n",
            "\n",
            "Test set: Avg. loss: 0.0025, Accuracy: 7874/10000 (79%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: Accuracy dropped to approximately 80% (79%) which means the proposed 'NegativePenaltySparseCategoricalCrossentropy' loss function workd as expected"
      ],
      "metadata": {
        "id": "IQ_hDrt6XqA-"
      }
    }
  ]
}